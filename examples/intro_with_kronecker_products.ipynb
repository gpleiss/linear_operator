{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import linear_operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Operator Kronecker Example\n",
    "\n",
    "We'll construct 2 (symmetric PSD) matrices: $\\mathbf A \\in \\mathbb R^{25 \\times 25}$ and $\\mathbf B \\in \\mathbb R^{100 \\times 100}$.\n",
    "\n",
    "The kronecker product $\\mathbf A \\otimes \\mathbf B$ is a $2500 \\times 2500$ matrix. Performing linear operations on $\\mathbf A \\otimes \\mathbf B$ can be fast if we take into account the structure afforded by the Kronecker product.\n",
    "\n",
    "By wrapping $\\mathbf A$ and $\\mathbf B$ in a `KroneckerProductLinearOperator`, we can perform algebraic operations on the Kronecker product in a structure-aware way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(25, 25)\n",
    "A = A @ A.T  # A 25 x 25 PSD matrix\n",
    "\n",
    "B = torch.randn(100, 100)\n",
    "B = B @ B.T  # A 100 x 100 PSD matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naively Computing $\\mathbf A \\otimes \\mathbf B$ Eigenvalues (Without LinearOperator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277 ms ± 805 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "kron = torch.kron(A, B)\n",
    "%timeit torch.linalg.eigh(kron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing $\\mathbf A \\otimes \\mathbf B$ Eigenvalues With LinearOperator\n",
    "\n",
    "We wil begin by wrapping `A` and `B` with the `to_linear_operator` function. After this, all math operations will take place with the linear operator abstraction, which will take into account the Kronecker product structure.\n",
    "\n",
    "If we are aware that $\\mathbf A \\otimes \\mathbf B$ has Kronecker structure, then we can compute eigenvalues efficiently. The linear_operator package keeps track of this structure. Calling `torch.kron` on LinearOperators returns a `KroneckerProductLinearOperator`, which codifies the Kronecker structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'linear_operator.operators.kronecker_product_linear_operator.KroneckerProductLinearOperator'>\n"
     ]
    }
   ],
   "source": [
    "A_lo = linear_operator.to_linear_operator(A)\n",
    "B_lo = linear_operator.to_linear_operator(B)\n",
    "kron = torch.kron(A_lo, B_lo)\n",
    "print(kron.__class__)  # It's not a torch.Tensor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.05 µs ± 22.2 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit torch.linalg.eigh(kron)  # It's much faster!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Lazy Evaluation of LinearOperators\n",
    "\n",
    "The linear operator package avoids explicitly instantiating any `LinearOperator` as a matrix. Any composition or decoration operation on `LinearOperators` returns another `LinearOperator` which specifies the structure of the operature through a tree-like object.\n",
    "\n",
    "For example: adding together two linear operators returns a `SumLinearOperator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_lo = linear_operator.operators.DiagLinearOperator(torch.randn(2500).abs())\n",
    "# This is a 2500 x 2500 diagonal matrix, represented as a LinearOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'linear_operator.operators.sum_linear_operator.SumLinearOperator'>\n"
     ]
    }
   ],
   "source": [
    "print((kron + D_lo).__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only methods that return `torch.Tensors` are those that perform some sort of reduction to a `LinearOperator`, such as a `matmul`, eigendecomposition, etc. The linear operator package attempts to perform all reductions in the most efficient way, given the structure of the operator.\n",
    "\n",
    "For example, note that `(kron + D_lo)` is the summation of a Kronecker product and a diagonal matrix. `matmul`s distribute across summations, and Kronecker products and diagonal matrices both have very efficient `matmul` implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.6 µs ± 101 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "vec = torch.randn(2500)\n",
    "\n",
    "# With LinearOperator - exploiting structure\n",
    "%timeit (kron + D_lo).matmul(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.9 ms ± 146 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Using dense torch.Tensor - ignoring structure\n",
    "%timeit (kron + D_lo).to_dense().matmul(vec)  # Much slower!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting LinearOperators to torch.Tensors\n",
    "\n",
    "If - at any point - we want to explicitly instantiate the dense matrix represented by a `LinearOperator`, we can call the `to_dense()` method.\n",
    "\n",
    "This is generally not recommended, since many `LinearOperators` are efficient (data-sparse) representations of large matrices. Calling `to_dense()` might easily create an object that eats up all available memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([2500, 2500])\n"
     ]
    }
   ],
   "source": [
    "dense = (kron + D_lo).to_dense()\n",
    "print(dense.__class__)\n",
    "print(dense.shape)  # A big matrix!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
